{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Search Engine\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping\n",
    "\n",
    "Using the crawler script, scrape the Marvel wiki website.\n",
    "\n",
    "```\n",
    "python ./crawler\n",
    "```\n",
    "\n",
    "***Note***\n",
    "\n",
    "\n",
    "The data from this action will be saved under the wiki folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Import all dependencies and create needed folders\n",
    "\n",
    "***Note***\n",
    "\n",
    "The following packages needs to be present on host machine.\n",
    "\n",
    "- requests\n",
    "- nltk\n",
    "- beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# download NLTK dependencies\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# create a new folder (corpus) to hold all cleaned text content from scrapped pages\n",
    "os.makedirs(os.path.join(os.getcwd(), \"corpus\"), exist_ok=True)\n",
    "# create a new folder (tables) to hold needed index tables\n",
    "os.makedirs(os.path.join(os.getcwd(), \"table\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "Using BeautifulSoup and regex:\n",
    "\n",
    "- discard all redundant parts of the HTML pages (eg. scripts, headers, footers etc)\n",
    "- strip all tags from the HTML pages so only text content remains\n",
    "- remove all punctuations\n",
    "- cast all text to lowercase\n",
    "\n",
    "***HINT***\n",
    "\n",
    "Below block takes about 1 minute to execute (due to size of scrapped docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents saved under corpus folder.\n",
      "System may print all text on a single line without word wrapping.\n"
     ]
    }
   ],
   "source": [
    "# Extract only the relevant text content from the html file into a text file\n",
    "def parser(html_path):\n",
    "    corpus_path = os.path.join(os.getcwd(),'corpus', os.path.basename(html_path).rsplit('.', 1)[0] + '.txt')\n",
    "\n",
    "    with open(html_path, 'r') as html_file:\n",
    "        with open(corpus_path, 'w') as corpus_file:\n",
    "            soup = BeautifulSoup(html_file.read(), 'html.parser')\n",
    "\n",
    "            # remove content table\n",
    "            soup.find(id='toc').decompose()\n",
    "            # remove superscripts\n",
    "            for sup in soup.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            # remove footnotes\n",
    "            while(soup.find('h2', text='See Also').find_next_sibling()):\n",
    "                soup.find('h2', text='See Also').find_next_sibling().decompose()\n",
    "            soup.find('h2', text='See Also').decompose()\n",
    "\n",
    "            # extract text content from current updated html content\n",
    "            corpus_text = soup.select('.mw-parser-output')[0].get_text(\"\\n\", strip=True)\n",
    "            # remove all punctuations from text content\n",
    "            corpus_text = corpus_text.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove all non alphanumeric characters\n",
    "            corpus_text = re.sub(r'[^A-Za-z0-9]+', ' ', corpus_text)\n",
    "            # cast all letters to lowercase\n",
    "            corpus_text = corpus_text.lower()\n",
    "\n",
    "            corpus_file.write(corpus_text)\n",
    "\n",
    "# get path reference to each scrapped wiki page and call parser function on each\n",
    "for html_path in glob.iglob(os.path.join(os.getcwd(), \"wiki/*.html\")):\n",
    "    parser(html_path)\n",
    "    print(f'\\r Cleaning --> {html_path}', end='')\n",
    "\n",
    "print('\\r' + 'All documents saved under corpus folder.\\nSystem may print all text on a single line without word wrapping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Building\n",
    "\n",
    "Using NLTK:\n",
    "- tokenise each cleaned up page \n",
    "- remove stop words\n",
    "- build document table\n",
    "- build vocabulary table\n",
    "- build postings table\n",
    "\n",
    "***Note***\n",
    "\n",
    "Cast arrays to set data structures for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of file path reference to all corpus (cleaned HTML pages)\n",
    "corpus_paths = [i for i in glob.iglob(os.path.join(os.getcwd(), 'corpus/*.txt'))]\n",
    "\n",
    "# tokenise and stem a corpus\n",
    "# parameter1 - string (corpus)\n",
    "def tokeniser(corpus):\n",
    "    tokens = []\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    sw = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    for term in set(nltk.word_tokenize(corpus)):\n",
    "        stemmed_term = ps.stem(term)\n",
    "        if stemmed_term not in sw:\n",
    "            tokens.append(stemmed_term)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document table builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and save docid table to file\n",
    "# return - the docids table as a dictionary\n",
    "def docids_table():\n",
    "    print('\\r Document table building ...', end='')\n",
    "\n",
    "    uid = 1\n",
    "    dic = {}\n",
    "    sorted_dic = collections.OrderedDict()\n",
    "\n",
    "    # map url to id\n",
    "    for corpus_path in corpus_paths:\n",
    "        url = os.path.basename(corpus_path).rsplit('.', 1)[0]\n",
    "        dic[url] = uid\n",
    "        uid += 1\n",
    "\n",
    "    # sort the dictionary alphabetically\n",
    "    for i in sorted(dic.keys()):\n",
    "        sorted_dic[i] = dic[i]\n",
    "\n",
    "\n",
    "    # save dictionary as json file\n",
    "    with open(os.path.join(os.getcwd(), 'table', 'docids.json'), 'w') as f:\n",
    "        f.write(json.dumps(sorted_dic))\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary table builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and save vocabulary table to file\n",
    "def vocabulary_table():\n",
    "    print('\\r Vocabulary table building ...', end='')\n",
    "\n",
    "    uid = 1\n",
    "    dic = {}\n",
    "    sorted_dic = collections.OrderedDict()\n",
    "\n",
    "    # map term to id\n",
    "    for corpus_path in corpus_paths:\n",
    "        with open(corpus_path, 'r') as f:\n",
    "            terms = tokeniser(f.read())\n",
    "            for term in terms:\n",
    "                if term not in dic:\n",
    "                    dic[term] = uid\n",
    "                    uid += 1\n",
    "\n",
    "    # sort the dictionary alphabetically\n",
    "    for i in sorted(dic.keys()):\n",
    "        sorted_dic[i] = dic[i]\n",
    "\n",
    "    # save dictionary as json file\n",
    "    with open(os.path.join(os.getcwd(), 'table', 'vocabulary.json'), 'w') as f:\n",
    "        f.write(json.dumps(sorted_dic))\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postings table builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and save postings table to file\n",
    "# parameter1 - docids table as a dictionary\n",
    "# parameter2 - vocabulary table as a dictionary\n",
    "def postings_table(docids_dic, vocabulary_dic):\n",
    "    print('\\r Postings table building ...', end='')\n",
    "\n",
    "    dic = {}\n",
    "    sorted_dic = collections.OrderedDict()\n",
    "    \n",
    "    # map term to (term id, document ids,  document frequency)\n",
    "    for corpus_path in corpus_paths:\n",
    "        url = os.path.basename(corpus_path).rsplit('.', 1)[0]\n",
    "        with open(corpus_path, 'r') as f:\n",
    "            terms = tokeniser(f.read())\n",
    "            for term in terms:\n",
    "                if term not in dic:\n",
    "                    dic[term] = { 'voc_id': vocabulary_dic[term], 'doc_ids': [ docids_dic[url] ], 'doc_freqs': [1] }\n",
    "                else:\n",
    "                    if docids_dic[url] not in dic[term]['doc_ids']:\n",
    "                        dic[term]['doc_freqs'].append(1)\n",
    "                    else:\n",
    "                        for i,j in enumerate(dic[term]['doc_ids']):\n",
    "                            if j == docids_dic[url]:\n",
    "                                dic[term]['doc_freqs'][i] = dic[term]['doc_freqs'][i] + 1\n",
    "                                \n",
    "                    doc_ids = [ *dic[term]['doc_ids'], *([docids_dic[url]] if docids_dic[url] not in dic[term]['doc_ids'] else []) ]\n",
    "                    dic[term] = { 'voc_id': vocabulary_dic[term], 'doc_ids': doc_ids, 'doc_freqs': dic[term]['doc_freqs'] }\n",
    "               \n",
    "    # sort the dictionary alphabetically\n",
    "    for i in sorted(dic.keys()):\n",
    "        sorted_dic[i] = dic[i]\n",
    "\n",
    "    # save dictionary as json file\n",
    "    with open(os.path.join(os.getcwd(), 'table', 'postings.json'), 'w') as f:\n",
    "        f.write(json.dumps(sorted_dic))\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigger tables build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All tables built and saved under table directory.\n",
      " A dictionary copy of each table has be set to memory for referencing."
     ]
    }
   ],
   "source": [
    "docids_dic = docids_table()\n",
    "vocabulary_dic = vocabulary_table()\n",
    "postings_dic = postings_table(docids_dic, vocabulary_dic)\n",
    "print('\\r All tables built and saved under table directory.\\n A dictionary copy of each table has be set to memory for referencing.', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single term extraction\n",
    "\n",
    "Enter a single term query to get the document list which contains query.\n",
    "\n",
    "The document list is sorted from most relevant to least relevant using term frequency.\n",
    "\n",
    "***Note***\n",
    "\n",
    "Next code block requires user input.\n",
    "\n",
    "Use the example ```name``` to get all documents since each Avenger has a name.\n",
    "\n",
    "***Testing***\n",
    "\n",
    "Copy and paste any single word from a particular avenger's page as query. And the function should return that avengers document url in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samuel_Wilson_(Earth-616)\n",
      "Reed_Richards_(Earth-616)\n",
      "Brian_Braddock_(Earth-616)\n",
      "Kamala_Khan_(Earth-616)\n",
      "Jennifer_Walters_(Earth-616)\n",
      "Peter_Parker_(Earth-616)\n",
      "Hercules_Panhellenios_(Earth-616)\n",
      "Anthony_Stark_(Earth-616)\n",
      "Wanda_Maximoff_(Earth-616)\n",
      "Miles_Morales_(Earth-1610)\n",
      "Steven_Rogers_(Earth-616)\n",
      "Jonathan_Hart_(Earth-616)\n",
      "Nadia_Van_Dyne_(Earth-616)\n",
      "Wendell_Vaughn_(Earth-616)\n",
      "James_Rhodes_(Earth-616)\n",
      "Janet_Van_Dyne_(Earth-616)\n",
      "Robert_Baldwin_(Earth-616)\n",
      "Clinton_Barton_(Earth-616)\n",
      "Thor_Odinson_(Earth-616)\n",
      "Pietro_Maximoff_(Earth-616)\n",
      "Roberto_Reyes_(Earth-616)\n",
      "Jane_Foster_(Earth-616)\n",
      "Gilgamesh_(Earth-616)\n",
      "Carol_Danvers_(Earth-616)\n",
      "T%27Challa_(Earth-616)\n",
      "Angelica_Jones_(Earth-616)\n",
      "Bruce_Banner_(Earth-616)\n",
      "Namor_McKenzie_(Earth-616)\n",
      "Anthony_Druid_(Earth-616)\n",
      "James_Howlett_(Earth-616)\n",
      "Venom_(Symbiote)_(Earth-616)\n",
      "Roberto_Da_Costa_(Earth-616)\n",
      "Eugene_Thompson_(Earth-616)\n",
      "Samuel_Alexander_(Earth-616)\n",
      "Shang-Chi_(Earth-616)\n",
      "Otto_Octavius_(Earth-616)\n",
      "Scott_Lang_(Earth-616)\n",
      "Stephen_Strange_(Earth-616)\n",
      "Susan_Storm_(Earth-616)\n",
      "Luke_Cage_(Earth-616)\n",
      "Captain_Universe_(Earth-616)\n",
      "William_Baker_(Earth-616)\n",
      "Samuel_Guthrie_(Earth-616)\n",
      "Eric_Masterson_(Earth-616)\n",
      "Jacques_Duquesne_(Earth-616)\n",
      "Noh-Varr_(Earth-200080)\n",
      "Henry_McCoy_(Earth-616)\n",
      "Maria_Hill_(Earth-616)\n",
      "Marcus_Milton_(Earth-13034)\n",
      "Robert_Frank_(Earth-616)\n",
      "Peter_Parker_(Tony_Richards)_(Earth-616)\n",
      "Monica_Rambeau_(Earth-616)\n",
      "Matthew_Hawk_(Earth-616)\n",
      "Natalia_Romanova_(Earth-616)\n",
      "Daisy_Johnson_(Earth-616)\n",
      "Dane_Whitman_(Earth-616)\n",
      "Jessica_Drew_(Earth-616)\n",
      "Greer_Grant_(Earth-616)\n",
      "Crystalia_Amaquelin_(Earth-616)\n",
      "Eros_(Earth-616)\n",
      "Anthony_Stark_(Earth-96020)\n",
      "Phoenix_Force_(Earth-616)\n",
      "Yondu_Udonta_(Earth-691)\n",
      "Imperial_Guard_(Earth-616)\n",
      "Simon_Williams_(Earth-616)\n",
      "Mar-Vell_(Earth-616)\n",
      "Heather_Douglas_(Earth-616)\n",
      "Stakar_Ogord_(Earth-691)\n",
      "Patricia_Walker_(Earth-616)\n",
      "Avenger_X_(Cressida)_(Earth-616)\n",
      "Eric_Brooks_(Earth-616)\n",
      "Richard_Jones_(Earth-616)\n",
      "Isabel_Kane_(Earth-616)\n",
      "Thaddeus_Ross_(Earth-616)\n",
      "Julia_Carpenter_(Earth-616)\n",
      "Vision_(Earth-616)\n",
      "James_Buchanan_Barnes_(Earth-616)\n",
      "Ororo_Munroe_(Earth-616)\n",
      "Jocasta_Pym_(Earth-616)\n",
      "Vance_Astro_(Earth-691)\n",
      "Genis-Vell_(Earth-98120)\n",
      "Eden_Fesi_(Earth-616)\n",
      "Aaron_Stack_(Earth-616)\n",
      "Tamara_Devoux_(Earth-616)\n",
      "Philip_Javert_(Earth-921)\n",
      "Maria_de_Guadalupe_Santiago_(Earth-616)\n",
      "Aleta_Ogord_(Earth-691)\n",
      "Delroy_Garrett_Jr._(Earth-616)\n",
      "Sersi_(Earth-616)\n",
      "Marrina_Smallwood_(Earth-616)\n",
      "Vance_Astrovik_(Earth-616)\n",
      "Zachary_Moonhunter_(Earth-616)\n",
      "Elvin_Haliday_(Earth-616)\n",
      "Dennis_Sykes_(Earth-616)\n",
      "Ex_Nihilo_(Earth-616)\n",
      "Dennis_Dunphy_(Earth-616)\n",
      "Mantis_(Brandt)_(Earth-616)\n",
      "Ulik_(Earth-616)\n",
      "Martinex_T%27Naga_(Earth-691)\n",
      "Barbara_Morse_(Earth-616)\n",
      "Abyss_(Ex_Nihilo%27s)_(Earth-616)\n",
      "Kelsey_Leigh_(Earth-616)\n",
      "Ravonna_Renslayer_(Earth-6311)\n",
      "Walter_Newell_(Earth-616)\n",
      "Rita_DeMara_(Earth-616)\n",
      "Magdalene_(Earth-9201)\n",
      "Adam_Blackveil_(Earth-616)\n",
      "Nicholette_Gold_(Earth-691)\n",
      "Maya_Lopez_(Earth-616)\n",
      "Giuletta_Nefaria_(3rd_Bio-Duplicate)_(Earth-616)\n",
      "Henry_Pym_(Earth-616)\n",
      "Sharra_Neramani_(Earth-616)\n",
      "Charlie-27_(Earth-691)\n",
      "Kevin_Connor_(Earth-616)\n",
      "Melissa_Gold_(Earth-98120)\n"
     ]
    }
   ],
   "source": [
    "def single_term(query):\n",
    "    dic = {}\n",
    "    try:\n",
    "        # remove punctuations, cast to lowercase and tokenise query\n",
    "        tokens = tokeniser(query.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "        # get match from postings table\n",
    "        post = postings_dic.get(tokens[0])\n",
    "    \n",
    "        for idx, post_doc_id in enumerate(post['doc_ids']):\n",
    "            for url, doc_id in docids_dic.items():\n",
    "                if post_doc_id == doc_id:\n",
    "                    dic[url] = post['doc_freqs'][idx]\n",
    "\n",
    "        # sort the dictionary by frequency and print results\n",
    "        ranked_result = [k for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "        if len(ranked_result):\n",
    "            print(*ranked_result, sep='\\n')\n",
    "            # uncomment to get a print out of the sorted rankings map\n",
    "            # print({k: v for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)})\n",
    "        else:\n",
    "            raise Exception()\n",
    "        \n",
    "    except Exception:\n",
    "        print('No matches found!')\n",
    "    \n",
    "query = input('Enter a single term query (eg. Name):').split()\n",
    "if len(query) == 1:\n",
    "    single_term(query[0].strip())\n",
    "elif len(query) < 1:\n",
    "    print('No term entered!')\n",
    "elif len(query) > 1:\n",
    "    print('More than one term entered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two term retrieval\n",
    "\n",
    "Enter a two terms query to get the document list which contains query.\n",
    "\n",
    "Each term should be a **single** world.\n",
    "\n",
    "Look at the complex term function for multi term queries.\n",
    "\n",
    "The document list is sorted from most relevant to least relevant using term frequency.\n",
    "\n",
    "***Note***\n",
    "\n",
    "Next code block requires user input.\n",
    "\n",
    "Use the example ```name AND history``` to get all documents since each Avenger has a name and history.\n",
    "\n",
    "***Testing***\n",
    "\n",
    "Copy and paste any two words from a particular avenger's page as query. And the function should return that avengers document url in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steven_Rogers_(Earth-616)\n",
      "Thor_Odinson_(Earth-616)\n",
      "Bruce_Banner_(Earth-616)\n",
      "James_Howlett_(Earth-616)\n",
      "Marcus_Milton_(Earth-13034)\n"
     ]
    }
   ],
   "source": [
    "def two_term(query1, query2):\n",
    "    dic = {}\n",
    "    get_doc_url = lambda lookup_id: [url for url, doc_id in docids_dic.items() if lookup_id == doc_id ][0]\n",
    "\n",
    "    try:\n",
    "        # remove punctuations, cast to lowercase and tokenise query\n",
    "        tokens1 = tokeniser(query1.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "        tokens2 = tokeniser(query2.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "        # get match from postings table\n",
    "        post1 = postings_dic.get(tokens1[0])\n",
    "        post2 = postings_dic.get(tokens2[0])\n",
    "\n",
    "        min_post = post1 if len(post1['doc_ids']) < len(post2['doc_ids']) else post2\n",
    "        max_post = post1 if len(post1['doc_ids']) > len(post2['doc_ids']) else post2\n",
    "        for i in range(len(min_post['doc_ids'])):\n",
    "            for j in range(len(max_post['doc_ids'])):\n",
    "                if min_post['doc_ids'][i] == max_post['doc_ids'][j]:\n",
    "                    url = get_doc_url(min_post['doc_ids'][i])\n",
    "                    dic[url] = min_post['doc_freqs'][i] + max_post['doc_freqs'][j]\n",
    "                    break\n",
    "        \n",
    "        # sort the dictionary by frequency and print results\n",
    "        ranked_result = [k for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "        if len(ranked_result):\n",
    "            print(*ranked_result, sep='\\n')\n",
    "            # uncomment to get a print out of the sorted rankings map\n",
    "            # print({k: v for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)})\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "    except Exception:\n",
    "        print('No matches found!')\n",
    "    \n",
    "query = input('Enter a two term query with \\'AND\\' delimeter (eg. History AND Powers):').split('AND')\n",
    "if len(query) == 2:\n",
    "    two_term(query[0].strip(), query[1].strip())\n",
    "elif len(query) < 2:\n",
    "    print('Missing \\'AND\\' delimeter!')\n",
    "elif len(query) > 2:\n",
    "    print('More than two terms entered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex queries\n",
    "\n",
    "Enter any number term query to get the document list which contains query.\n",
    "\n",
    "The document list is sorted from most relevant to least relevant using term frequency.\n",
    "\n",
    "***Note***\n",
    "\n",
    "Next code block requires user input.\n",
    "\n",
    "Use the example ```who started the war between Minya and Thebe``` to get Hercules_Panhellenios_(Earth-616) [document id = 35] as the result document.\n",
    "\n",
    "This function tries to find the document with the most number of terms withing give query. So you can use whole paragraphs from the document HTML and it should\n",
    "return an exact match.\n",
    "\n",
    "***Testing***\n",
    "\n",
    "Copy and paste a string from a particular avenger's page as query. The function should return that avengers document url in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter_Parker_(Earth-616)\n",
      "Hercules_Panhellenios_(Earth-616)\n",
      "Anthony_Stark_(Earth-616)\n",
      "Bruce_Banner_(Earth-616)\n",
      "James_Howlett_(Earth-616)\n",
      "Eugene_Thompson_(Earth-616)\n",
      "Shang-Chi_(Earth-616)\n",
      "Stephen_Strange_(Earth-616)\n",
      "Steven_Rogers_(Earth-616)\n",
      "Reed_Richards_(Earth-616)\n",
      "Brian_Braddock_(Earth-616)\n",
      "Robert_Baldwin_(Earth-616)\n",
      "Pietro_Maximoff_(Earth-616)\n",
      "Dennis_Sykes_(Earth-616)\n",
      "Natalia_Romanova_(Earth-616)\n",
      "Jessica_Drew_(Earth-616)\n",
      "Wanda_Maximoff_(Earth-616)\n",
      "Miles_Morales_(Earth-1610)\n",
      "Barbara_Morse_(Earth-616)\n",
      "Venom_(Symbiote)_(Earth-616)\n",
      "Eric_Brooks_(Earth-616)\n",
      "Samuel_Alexander_(Earth-616)\n",
      "Thaddeus_Ross_(Earth-616)\n",
      "Otto_Octavius_(Earth-616)\n",
      "Susan_Storm_(Earth-616)\n",
      "Julia_Carpenter_(Earth-616)\n",
      "Luke_Cage_(Earth-616)\n",
      "Captain_Universe_(Earth-616)\n",
      "James_Buchanan_Barnes_(Earth-616)\n",
      "Ororo_Munroe_(Earth-616)\n",
      "William_Baker_(Earth-616)\n",
      "Nadia_Van_Dyne_(Earth-616)\n",
      "Eric_Masterson_(Earth-616)\n",
      "Samuel_Wilson_(Earth-616)\n",
      "Janet_Van_Dyne_(Earth-616)\n",
      "Clinton_Barton_(Earth-616)\n",
      "Henry_McCoy_(Earth-616)\n",
      "Maria_Hill_(Earth-616)\n",
      "Marrina_Smallwood_(Earth-616)\n",
      "Peter_Parker_(Tony_Richards)_(Earth-616)\n",
      "Monica_Rambeau_(Earth-616)\n",
      "Jane_Foster_(Earth-616)\n",
      "Greer_Grant_(Earth-616)\n",
      "Carol_Danvers_(Earth-616)\n",
      "T%27Challa_(Earth-616)\n",
      "Angelica_Jones_(Earth-616)\n",
      "Phoenix_Force_(Earth-616)\n",
      "Mar-Vell_(Earth-616)\n",
      "Scott_Lang_(Earth-616)\n"
     ]
    }
   ],
   "source": [
    "def complex_term(query):\n",
    "    dic = {}\n",
    "    posts = []\n",
    "    max_post_ref = (0, 0) # holds a size, index reference for the post with maximum document ids\n",
    "    get_doc_url = lambda lookup_id: [url for url, doc_id in docids_dic.items() if lookup_id == doc_id ][0]\n",
    "\n",
    "    try:\n",
    "        # remove punctuations, cast to lowercase and tokenise query\n",
    "        tokens = tokeniser(query.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "        # group all postings results from term searhes into a list\n",
    "        for idx, term in enumerate(tokens):\n",
    "            post = postings_dic.get(term)\n",
    "            if post:\n",
    "                doc_ids_size = len(post['doc_ids'])\n",
    "                max_post_ref = (doc_ids_size, idx) if doc_ids_size > max_post_ref[0] else (max_post_ref[0], max_post_ref[1])\n",
    "                posts.append(post)\n",
    "        \n",
    "        for i in range(max_post_ref[0]):\n",
    "            freq = 0\n",
    "            post_index = 0\n",
    "            intersection_count = 0\n",
    "            while post_index < len(posts):\n",
    "                for k in range(len(posts[post_index]['doc_ids'])):\n",
    "                    if posts[post_index]['doc_ids'][k] == posts[max_post_ref[1]]['doc_ids'][i]:\n",
    "                        intersection_count += 1\n",
    "                        freq += posts[post_index]['doc_freqs'][k]\n",
    "                        break\n",
    "                post_index += 1\n",
    "            if intersection_count == len(posts):\n",
    "                url = get_doc_url(posts[max_post_ref[1]]['doc_ids'][i])\n",
    "                dic[url] = freq\n",
    "    \n",
    "        # sort the dictionary by frequency and print results\n",
    "        ranked_result = [k for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "        if len(ranked_result):\n",
    "            print(*ranked_result, sep='\\n')\n",
    "            # uncomment to get a print out of the sorted rankings map\n",
    "            # print({k: v for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)})\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "    except Exception:\n",
    "        print('No matches found!')\n",
    "\n",
    "query = input('Enter any string as query.\\n(eg. \\n1. Peter Benjamin Parker was born in Queens) or \\n2. Benjamin AND Parker AND born:')\n",
    "complex_term(query)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ae2065f471b3674c5234b268a4ccc8ff3033d13d6976ef2eebe39958ab741ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ir_cw_1-uQB4q2Av': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
